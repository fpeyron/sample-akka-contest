akka {

  loglevel        = INFO
  stdout-loglevel = INFO

  # Enable metrics extension in akka-cluster-metrics.
  extensions  = ["akka.cluster.metrics.ClusterMetricsExtension"]

  # Enable cluster extension.
  extensions  = ["de.heikoseeberger.constructr.ConstructrExtension"]

  actor {
    //provider  = "akka.cluster.ClusterActorRefProvider"
    provider  = "cluster"

    serializers {
      kryo = "com.romix.akka.serialization.kryo.KryoSerializer"
    }
    serialization-bindings {
      "java.io.Serializable" = kryo
    }

    kryo {
      # https://github.com/romix/akka-kryo-serialization for more information about configuration
      type = "graph"
      idstrategy = "default"
      buffer-size = 4096
      max-buffer-size = -1
      use-manifests = false
      use-unsafe = false
      post-serialization-transformations = "off"
      implicit-registration-logging = false
      kryo-trace = false
      resolve-subclasses = false
    }
  }

  remote {
    enabled-transports = ["akka.remote.netty.tcp"]
    log-remote-lifecycle-events = on
    netty.tcp {

      bind-hostname = 127.0.0.1
      bind-port     = 2550
      //bind-hostname = ${?SERVICE_AKKA_HOST}
      //bind-port     = ${?SERVICE_AKKA_PORT}

      hostname      = localhost
      port          = 2550
      hostname      = ${?SERVICE_AKKA_HOST}
      port          = ${?SERVICE_AKKA_PORT}
    }
  }
  cluster {
    seed-node-timeout = 2s

    # Should the 'leader' in the cluster be allowed to automatically mark
    # unreachable nodes as DOWN after a configured time of unreachability?
    # Using auto-down implies that two separate clusters will automatically be
    # formed in case of network partition.
    #
    # Don't enable this in production, see 'Auto-downing (DO NOT USE)' section
    # of Akka Cluster documentation.
    #
    # Disable with "off" or specify a duration to enable auto-down.
    # If a downing-provider-class is configured this setting is ignored.
    auto-down-unreachable-after = 3s

    # Time margin after which shards or singletons that belonged to a downed/removed
    # partition are created in surviving partition. The purpose of this margin is that
    # in case of a network partition the persistent actors in the non-surviving partitions
    # must be stopped before corresponding persistent actors are started somewhere else.
    # This is useful if you implement downing strategies that handle network partitions,
    # e.g. by keeping the larger side of the partition and shutting down the smaller side.
    # It will not add any extra safety for auto-down-unreachable-after, since that is not
    # handling network partitions.
    # Disable with "off" or specify a duration to enable.
    #down-removal-margin = 3s

    unreachable-nodes-reaper-interval = 250ms

    failure-detector {
      acceptable-heartbeat-pause  = 4s
      hearbeat-interval           = 250ms
      threshold                   = 4.0
    }

    singleton {

      # When a node is becoming oldest it sends hand-over request to previous oldest,
      # that might be leaving the cluster. This is retried with this interval until
      # the previous oldest confirms that the hand over has started or the previous
      # oldest member is removed from the cluster (+ akka.cluster.down-removal-margin).
      hand-over-retry-interval = 1s

      # The number of retries are derived from hand-over-retry-interval and
      # akka.cluster.down-removal-margin (or ClusterSingletonManagerSettings.removalMargin),
      # but it will never be less than this property.
      min-number-of-hand-over-retries = 5
    }

    singleton-proxy {

      # Interval at which the proxy will try to resolve the singleton instance.
      singleton-identification-interval = 1s

      # If the location of the singleton is unknown the proxy will buffer this
      # number of messages and deliver them when the singleton is identified.
      # When the buffer is full old messages will be dropped when new messages are
      # sent via the proxy.
      # Use 0 to disable buffering, i.e. messages will be dropped immediately if
      # the location of the singleton is unknown.
      # Maximum allowed buffer size is 10000.
      buffer-size = 1000
    }

    # Disable legacy metrics in akka-cluster
    metrics.enabled=off
    http {
      management {
        hostname      = "0.0.0.0"
        port          = 5000
      }
    }
  }
}

api {
  http {
    hostname    = 0.0.0.0
    port        = 8080
    timeout     = 2000
  }
}

constructr {
  coordination {
    class-name = com.github.everpeace.constructr.coordination.redis.RedisCoordination
    host       = localhost
    port       = 6379
    host       = ${?REDIS_HOST}
    port       = ${?REDIS_PORT}
    redis {
      password   = ${?REDIS_PASSWORD}
      db         = ${?REDIS_DATABASE}
    }
  }
  coordination-timeout      = 3 seconds   // Maximum response time for coordination service (e.g. etcd)
  join-timeout              = 15 seconds  // Might depend on cluster size and network properties
  max-nr-of-seed-nodes      = -1          // Any nonpositive value means Int.MaxValue
  //nr-of-retries           = 2           // Nr. of tries are nr. of retries + 1
  //refresh-interval        = 5 seconds   // TTL is refresh-interval * ttl-factor
  //retry-delay             = 1 seconds   // Give coordination service (e.g. etcd) some delay before retrying
  //ttl-factor              = 2.0         // Must be greater or equal 1 + ((coordination-timeout * (1 + nr-of-retries) + retry-delay * nr-of-retries)/ refresh-interval)!
  //ignore-refresh-failures = false       // Ignore failures once machine is already in "Refreshing" state. It prevents from FSM being terminated due to exhausted number of retries.
}

slick {
  profile = "slick.jdbc.MySQLProfile$"
  db {
    host = localhost
    host = ${?MYSQL_HOST}
    port = "3306"
    port = ${?MYSQL_PORT}
    user = "root"
    user = ${?MYSQL_USER}
    password = "root"
    password = ${?MYSQL_PASSWORD}
    database = "game"
    database = ${?MYSQL_DATABASE}
    driver = "com.mysql.cj.jdbc.Driver"
    url = "jdbc:mysql://"${slick.db.host}":"${slick.db.port}"/"${slick.db.database}"?cachePrepStmts=true&cacheCallableStmts=true&cacheServerConfiguration=true&useLocalSessionState=true&elideSetAutoCommits=true&alwaysSendSetIsolation=false&enableQueryTimeouts=false&connectionAttributes=none&verifyServerCertificate=false&useSSL=false&useUnicode=true&useLegacyDatetimeCode=false&serverTimezone=UTC&rewriteBatchedStatements=true"
    connectionTestQuery = "SELECT 1"
    numThreads = 10
    queueSize=1000
  }
}

jdbc-journal {
  slick = ${slick}
  slick.db.numThreads = 20
  slick.db.maxConnections = 100
  slick.db.minConnections = 1
  slick.db.connectionTimeout = 1800000 // 30 minutes
  recovery-event-timeout = 60m

  event-adapters {
    test-write-event-adapter = "akka.persistence.jdbc.query.EventAdapterTest$TestWriteEventAdapter"
    test-read-event-adapter  = "akka.persistence.jdbc.query.EventAdapterTest$TestReadEventAdapter"
  }

  event-adapter-bindings {
    "akka.persistence.jdbc.query.EventAdapterTest$Event"            = test-write-event-adapter
    "akka.persistence.jdbc.query.EventAdapterTest$TaggedEvent"      = test-write-event-adapter
    "akka.persistence.jdbc.query.EventAdapterTest$TaggedAsyncEvent" = test-write-event-adapter
    "akka.persistence.jdbc.query.EventAdapterTest$EventAdapted"     = test-read-event-adapter
  }
}

# the akka-persistence-snapshot-store in use
jdbc-snapshot-store {
  slick = ${slick}
  slick.db.numThreads = 20
  slick.db.maxConnections = 100
  slick.db.minConnections = 1
  slick.db.connectionTimeout = 1800000 // 30 minutes

  tables {
    snapshot {
      tableName = "EVT_SNAPSHOT"
      columnNames {
        persistenceId = "persistence_id"
        sequenceNumber = "sequence_number"
        created = "created"
        snapshot = "snapshot"
      }
    }
  }
}

# the akka-persistence-query provider in use
jdbc-read-journal {
  refresh-interval = "10ms"
  max-buffer-size = "250"
  slick = ${slick}
  slick.db.numThreads = 20
  slick.db.maxConnections = 100
  slick.db.minConnections = 1
  slick.db.connectionTimeout = 1800000 // 30 minutes

  tables {
    journal {
      tableName = "EVT_JOURNAL"
      columnNames {
        ordering = "ordering"
        deleted = "deleted"
        persistenceId = "persistence_id"
        sequenceNumber = "sequence_number"
        created = "created"
        tags = "tags"
        message = "message"
      }
    }
  }
}